{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "##Importing Libraries\n",
    "## Libraries are ensemble of modules\n",
    "\n",
    "import numpy as np ##allows to work with arrays\n",
    "import pandas as pd ##helps us to import data sets and also create us matrix  \n",
    "## Importing Dataset\n",
    "\n",
    "dataset = pd.read_csv(\"Data.csv\") ##we call pandas so then its function with \".\" then, read_csv is a pandas funciton , dataframe = certain form of data, then we put the name of the data set inside the brackets\n",
    "##Now we have to create 2 new entities - 1 is the matrix of features and the another is the dependent variable vector\n",
    "##Dependent variable is the last column, matrix of features are the rest of the columns\n",
    "x = dataset.iloc[:,:-1].values\n",
    "## x is for matrix of feature, iloc[] is a pandas function which stands for location index, it takes the indexes of the colums/rows we want to extract from the data set\n",
    "## To take all the rows of the column use the colon inside which means no starting or ending, then to specify the columns we need put a comma after the colon\n",
    "## To take all the columns except the last one add a new range which is \":-1\", -1 in python means last column\n",
    "## values is for taking all the values from the rows and columns\n",
    "y  = dataset.iloc[:,-1].values\n",
    "## y is for dependent variable, I put -1 for columns section instead of : because I only need that column\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "## Taking care of missing data\n",
    "\n",
    "##Missing data can cause some error in training ml models\n",
    "##we can delete is , if dataset is large and missing data is very little\n",
    "##or we can replace the missing value by the average all the available value\n",
    "\n",
    "##scikit learn is a library used for data preprocessing\n",
    "##we will be using it to handle missing data, to do this the class we will be using from it is called simpleimputer,\n",
    "##we will first import he class then we will create an instance/object of that class to exactly replace the missing value byt he average of the other values,then we will have an updated dataset with new set of values\n",
    "from sklearn.impute import SimpleImputer ##inorder the acceses a module we have to add a \".\", simpleimputer class belongs to a module called \"impute\", then we import the class\n",
    "\n",
    "##Since we are about to create a new object we need a new variable\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean') ##imputer is the object of the class simpleimputer\n",
    "##Then we call the respective class with tis name and then by adding \"()\" to call the class and assign it inside the object\n",
    "##Now we can enter the right arguements\n",
    "##To repalce first we have to specify what are we replacing\n",
    "## So we use the arguement called missing values  to specify it then we use np.nan to deal with arrays thenwith comma we use the argument strategy which equals to mean in quotation \n",
    "## we can also use median instead of mean.\n",
    "##Till now imputer is just a object, we didn't connect anythign yet to our matrix of features\n",
    "imputer.fit(x[:, 1:3])##Fit method looks for missing vales and compute mean function, then to do the replacement we call another method called transform \n",
    "##1 is age column, 2 is salary column\n",
    "##first call the object and then call the method called fit, the arguemet for fit method is only numerical values not the text or string\n",
    "##we give x, because we need data from there\n",
    "##This connects imputer to matix of features\n",
    "##Now call transform method from imputer object, this method does the relacement\n",
    "imputer.transform(x[:, 1:3]) ## this transform method returns the new updated version of x with the 2 replacements made\n",
    "##Since transform method replaces to update x we take the updated colums and change it by what will it be returned by imputer function \n",
    "x[:,1:3] = imputer.transform(x[:, 1:3])\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encoding Categorical Data\n",
    "\n",
    "##If we see  our data set we can find one column with categories like France, spain or germany\n",
    "##It would be difficult for the ml models to co-relate between these features and the outcome, so we encode them into numbers\n",
    "##So we encode France into 0, spain into and germany into 2. so our ml model can understand the numerical order in these countries. But it may also interpret this this order matters but that's not the case.\n",
    "##There is no relationship order between these  countries, we want our madel to avoid those so that it don't misinterpret the outcome we want to predict.\n",
    "##The thing we can do is  one-hot encoding. This consited of turning this country column into 3 different columns as it has 3 categories(France,spain,germany)\n",
    "##OHE consistes of creating binary vectors for each country.\n",
    "##We will also replace the dependent variable values by 0 and 1\n",
    "\n",
    "##We will use 2 class\n",
    "##columntranformer class from compose module in scikit library\n",
    "##another is one hot encoded class from the preprocessing module of the same scikit learn library\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "##Now create an object for the column transformer class\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder= 'passthrough') ##We call the class to create an instance or an object, inside the paranthesis we have t0 2 enter arguements\n",
    "##These arguements are first transformers where we specify what kind of transformation we want to do, which index we want to transform and the other once is remainder which will specify the columns where we won't be applying the transform.\n",
    "##Now for the transformers we have to specify 3 things, first the kind of transformation (encoding), what kind of encoding (one hot, we enter the class name exactly with brackets), index of the columns we want to encode (Country column). we enter them in new square brackets\n",
    "##We input them in a pair of square brackets and curved brackets(Tuple)\n",
    "##for remainder we have to specify the code name that is \"passthrough\", this prevent other columns from ine hot encoding \n",
    "##The column transformer class has a method called fit transform which fits and transforms together to connect the object with the matrix feature\n",
    "x = np.array(ct.fit_transform(x))\n",
    "## we update the matrix of features by assigning it to x\n",
    "##fit transform method returns x as a numpy array so it is absolutely necessary to enter x as a numpy array\n",
    "##So above we have to force it fit transform to return as a numpy array we can do it by calling numpy by shortcut np,then.array which take the output of the fit transform method as it's input\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "##Now is the dependent variable section we want to encode the values of x and y as 0 and 1\n",
    "##We can do it by using another class called label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder() ##Since we are just transforming in one single columnwe don't want to enter anythign inside\n",
    "y = le.fit_transform(y) ##Since this is a dependent variable this doesn't has to be a numpy array\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Splitting the data set into training set and tesing set\n",
    "\n",
    "##Always apply feature scaling after splitting the data set itno training and test set\n",
    "##In training set we train the machine learning model on existing observations\n",
    "##In test set we evaluate the performance of the model on new observations\n",
    "##Feature scaling simply consists of  scaling all your variables/features to make sure they take values in the same scale\n",
    "##Why should we apply it after splitting?? It is simply because the test set is a brand new set, and feature scaling is something where we take the mean and standard deviation of the features\n",
    "##and if we apply the feature scaling before splitting we would end with the mean and standard deviation of all the values including test set\n",
    "##Test set is something like future data, apllying feature scaling would cause some information leakage \n",
    "\n",
    "##We are going to split it by a function called train_test_split from a module called model selection in scikit learn library\n",
    "##This function performs 4 different sets which is x and y for training set and x and y for test set \n",
    "##x(matrix of features), y(dependent variable)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1)  ## Matrix of feature sof training set, test set , dependent variable of test set and training set \n",
    "##we first enter the x and y as inputs as the arguments and for the other 2 arguements the split size(we need lot of observations in training set and comparatively less in test set) Recommended size of the split is 80% in training set and 20% in test set\n",
    "##the final arguement is random_state as the observations will be split into random splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n",
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "##Feature Scaling \n",
    "\n",
    "##We won't apply this for all ml models \n",
    "##2 main feature scaling tehniques are standardisation and normalisation\n",
    "#Standardisation - consists od subtracting each value of the feature byt he mean fo the values of the feature and diving by standard deviation (Square root of the variance). all the values remain fron -3 to +3\n",
    "##Normalisation - subtracting each value of the feature by the minimum valueof the feature ad=nd then dividing by the difference of the maximum value of the feature and the minimum value of the feature, values will be from 0 and 1\n",
    "\n",
    "## normalisation is recommended if we have normal distribution in most of our features \n",
    "## standardisation works all the time. Therefore, we can use this everywhere and it will work\n",
    "\n",
    "##We will do with scikit learn and with the class called standard scalar  from preprocessing module which performs standardisation\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()##we din't have any arguements to include becausewe will justv need the mean and standard deviation\n",
    "##Only apply feature scaling to numerical values and not dummy variables\n",
    "x_train[:,3:] = sc.fit_transform(x_train[:,3:])##I give index 3 because the index for age stars from 3 as the first 3 indexes 0 to 2 are binary encoded values of countries\n",
    "print(x_train)\n",
    "x_test[:,3:] = sc.transform(x_test[:,3:]) ##We only transform in test set and don't calculate the mean and standard deviation for test set\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
